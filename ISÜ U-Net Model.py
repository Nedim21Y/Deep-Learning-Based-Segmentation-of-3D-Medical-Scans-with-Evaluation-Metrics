# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PW4RD4rfUl8ydyqmFXe-hEk9Xz3JuQ5K
"""

import os
import glob
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import time


# config.py content

DATA_DIR = "/content/drive/MyDrive/Colab.data/tubitk folder"
OUTPUT_FOLDER_NAME = "SimpleOutput_Segmentation"
OUTPUT_DIR = os.path.join("/content/drive/MyDrive", OUTPUT_FOLDER_NAME) # Dynamically create Google Drive output path

# --- 2. MODEL & TRAINING PARAMETERS ---
# The shape of your .npy arrays (Height, Width, Depth)
INPUT_SHAPE = (128, 128, 64)
BATCH_SIZE = 1
EPOCHS = 30
LEARNING_RATE = 1e-4
NUM_CLASSES = 1 # Set to 1 for binary segmentation (background vs. foreground).
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_SAVE_NAME = "light_unet_best.pth"

# metrics.py content (New Section)

def dice_score(preds, targets, smooth=1e-6):
    """
    Calculates the Dice Similarity Coefficient (DSC).
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: Dice score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum()

    dice = (2. * intersection + smooth) / (union + smooth)
    return dice

def iou_score(preds, targets, smooth=1e-6):
    """
    Calculates the Intersection Over Union (IoU) or Jaccard Index.
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: IoU score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum() - intersection # Union = A + B - Intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou

# You can add more metrics here if needed, e.g., accuracy, sensitivity, specificity
def calculate_metrics(outputs, masks):
    """
    Calculates Dice and IoU scores for a batch of predictions and ground truths.
    Args:
        outputs (torch.Tensor): Raw model outputs (logits).
        masks (torch.Tensor): Ground truth masks.
    Returns:
        tuple: (average_dice, average_iou)
    """
    # Binarize predictions
    preds = (torch.sigmoid(outputs) > 0.5).float()

    batch_dice = []
    batch_iou = []

    for i in range(preds.shape[0]): # Iterate over batch dimension
        d = dice_score(preds[i], masks[i])
        j = iou_score(preds[i], masks[i])
        batch_dice.append(d.item())
        batch_iou.append(j.item())

    return np.mean(batch_dice), np.mean(batch_iou)


# ==============================================================================
# dataset.py content
# ==============================================================================
def get_data_splits(data_dir):
    """
    Finds image and mask .npy files, pairs them, and splits them into
    training, validation, and test sets.

    Args:
        data_dir (str): The path to the directory containing the .npy files.

    Returns:
        tuple: A tuple containing three lists: train_files, val_files, test_files.
               Each list contains (image_path, mask_path) tuples.
    """
    print("\n--- Finding and splitting data files ---")
    print(f"DEBUG: 'data_dir' received: '{data_dir}'")

    # Step 1: Verify the data directory exists
    if not os.path.isdir(data_dir):
        print(f"ERROR: The directory '{data_dir}' does not exist.")
        raise FileNotFoundError(f"Directory '{data_dir}' not found. Please ensure the path is correct and exists on your system.")

    # Step 2: Find all image files using a glob pattern
    search_pattern = os.path.join(data_dir, "*_cropped_img.npy")
    print(f"DEBUG: Glob search pattern: '{search_pattern}'")

    all_image_files = sorted(glob.glob(search_pattern))
    print(f"Found {len(all_image_files)} image files.")
    print(f"DEBUG: All image files found by glob: {all_image_files[:5]}")

    if not all_image_files:
        raise FileNotFoundError(f"No *_cropped_img.npy files found in '{data_dir}' using pattern '{search_pattern}'. Check file names or directory.")

    # Step 3: Generate expected mask file paths from image file paths
    all_mask_files = [f.replace("_img.npy", "_mask.npy") for f in all_image_files]
    print(f"Generated {len(all_mask_files)} expected mask file paths.")

    print("🧾 Sample image files found:", all_image_files[:2])
    print("🧾 Sample mask files expected:", all_mask_files[:2])

    # Step 4: Pair image and mask files, checking for mask file existence
    paired_files = []
    for img, mask in zip(all_image_files, all_mask_files):
        if os.path.exists(mask):
            paired_files.append((img, mask))
        else:
            print(f" Missing mask file: {mask}")

    print(f"DEBUG: Number of valid image-mask pairs found: {len(paired_files)}")

    if not paired_files:
        raise FileNotFoundError(f"No valid image-mask pairs found in '{data_dir}'. Check file names.")

    # Step 5: Shuffle the paired files and split into train, validation, and test sets
    random.seed(42) # For reproducibility
    random.shuffle(paired_files)

    total_files = len(paired_files)
    train_split_idx = int(total_files * 0.8)
    val_split_idx = int(total_files * 0.9)

    train_files = paired_files[:train_split_idx]
    val_files = paired_files[train_split_idx:val_split_idx]
    test_files = paired_files[val_split_idx:]

    print(f"Total pairs: {total_files} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}")
    return train_files, val_files, test_files


class SimpleNpyDataset(Dataset):
    """
    A PyTorch Dataset class to load pre-processed .npy image and mask files.
    It handles loading, squeezing (if necessary), casting to float32,
    and reshaping to PyTorch's (C, D, H, W) format for 3D data, or (C, H, W) for 2D.
    """
    def __init__(self, file_pairs):
        """
        Initializes the dataset with lists of image and mask file paths.

        Args:
            file_pairs (list): A list of (image_path, mask_path) tuples.
        """
        self.image_paths = [pair[0] for pair in file_pairs]
        self.mask_paths = [pair[1] for pair in file_pairs]
        print(f"🔍 Initialized dataset with {len(self.image_paths)} samples.")

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.image_paths)

    def __getitem__(self, idx):
        """
        Loads and processes an image-mask pair at a given index.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: A tuple containing the processed image tensor and mask tensor.
                   Expected output shape for 3D data is (C, D, H, W).
        """
        # Load numpy arrays
        img = np.load(self.image_paths[idx])
        mask = np.load(self.mask_paths[idx])

        # Squeeze out singleton dimensions, e.g., (1, D, H, W) -> (D, H, W)
        if img.ndim == 4 and img.shape[0] == 1:
            img = np.squeeze(img, axis=0)
        if mask.ndim == 4 and mask.shape[0] == 1:
            mask = np.squeeze(mask, axis=0)

        # Binarize mask and ensure float32 type
        mask = (mask > 0).astype(np.float32)

        # Convert numpy arrays to PyTorch tensors and ensure float type
        img_tensor = torch.from_numpy(img).float()
        mask_tensor = torch.from_numpy(mask).float()

        # Reshape for PyTorch: (D, H, W) -> (C, D, H, W) for 3D grayscale
        if img_tensor.ndim == 3: # Assuming it's (Depth, Height, Width) for 3D grayscale
            img_tensor = img_tensor.unsqueeze(0)
        elif img_tensor.ndim == 2: # Assuming (H, W) for 2D grayscale
            img_tensor = img_tensor.unsqueeze(0)

        if mask_tensor.ndim == 3:
            mask_tensor = mask_tensor.unsqueeze(0)
        elif mask_tensor.ndim == 2:
            mask_tensor = mask_tensor.unsqueeze(0)

        return img_tensor, mask_tensor


# model.py content

def double_conv_block(in_channels, out_channels):
    """A block of two 3D convolutions with instance norm and ReLU."""
    return nn.Sequential(
        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
    )

class LightUNet3D(nn.Module):
    """A lighter version of the 3D U-Net for faster training."""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # Encoder
        self.enc1 = double_conv_block(in_channels, 16)
        self.pool1 = nn.MaxPool3d(2)
        self.enc2 = double_conv_block(16, 32)
        self.pool2 = nn.MaxPool3d(2)
        # Bottleneck
        self.bottleneck = double_conv_block(32, 64)
        # Decoder
        self.up2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.dec2 = double_conv_block(64, 32)
        self.up1 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)
        self.dec1 = double_conv_block(32, 16)
        # Output
        self.out_conv = nn.Conv3d(16, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bottleneck(self.pool2(e2))

        d2 = self.up2(b)
        if d2.shape[-3:] != e2.shape[-3:]:
            diffD = e2.shape[2] - d2.shape[2]
            diffH = e2.shape[3] - d2.shape[3]
            diffW = e2.shape[4] - d2.shape[4]
            e2 = e2[:, :, diffD // 2 : e2.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e2.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e2.shape[4] - diffW + diffW // 2]
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        if d1.shape[-3:] != e1.shape[-3:]:
            diffD = e1.shape[2] - d1.shape[2]
            diffH = e1.shape[3] - d1.shape[3]
            diffW = e1.shape[4] - d1.shape[4]
            e1 = e1[:, :, diffD // 2 : e1.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e1.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e1.shape[4] - diffW + diffW // 2]
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        return self.out_conv(d1)

# ==============================================================================
# train.py content
# ==============================================================================
def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_save_path):
    """Main training and validation loop."""
    print("\n--- Starting Model Training ---")
    best_val_loss = float('inf')

    for epoch in range(epochs):
        start_time = time.time()

        # --- Training phase ---
        model.train()
        total_train_loss = 0

        for imgs, masks in train_loader:
            imgs, masks = imgs.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # --- Validation phase ---
        model.eval()
        total_val_loss = 0
        total_val_dice = 0
        total_val_iou = 0

        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs, masks = imgs.to(device), masks.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, masks)
                total_val_loss += loss.item()

                # Calculate metrics for validation
                dice, iou = calculate_metrics(outputs, masks)
                total_val_dice += dice
                total_val_iou += iou

        avg_val_loss = total_val_loss / len(val_loader)
        avg_val_dice = total_val_dice / len(val_loader)
        avg_val_iou = total_val_iou / len(val_loader)

        # --- Logging and Saving ---
        elapsed_time = time.time() - start_time
        print(f"Epoch {epoch+1:02}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Val Dice: {avg_val_dice:.4f} | "
              f"Val IoU: {avg_val_iou:.4f} | "
              f"Time: {elapsed_time:.2f}s")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f"  🎉 New best model saved with Val Loss: {best_val_loss:.4f}")

    print("\n--- Model Training Finished ---")

# ==============================================================================
# predict.py content
# ==============================================================================
# ==============================================================================
# predict.py content
# ==============================================================================
def predict_and_visualize(model, loader, device, model_path, output_dir):
    """Runs prediction on a single test sample and saves a visualization."""
    print("\n--- Running Prediction and Visualization ---")

    # Load the best model weights
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Successfully loaded model from: {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}. Cannot perform prediction.")
        return
    except Exception as e:
        print(f"Error loading model state_dict: {e}")
        return

    model.eval() # Set model to evaluation mode

    total_test_dice = 0
    total_test_iou = 0
    num_samples_tested = 0

    with torch.no_grad():
        # Iterate through the entire test loader for comprehensive metric calculation
        for i, (current_img_batch, current_mask_batch) in enumerate(loader): # Renamed for clarity
            # Move tensors to the specified device (CPU/GPU)
            current_img_batch, current_mask_batch = current_img_batch.to(device), current_mask_batch.to(device)

            output_logits = model(current_img_batch)

            # Calculate metrics
            dice, iou = calculate_metrics(output_logits, current_mask_batch)
            total_test_dice += dice
            total_test_iou += iou
            num_samples_tested += 1

            # Only visualize the first sample
            if i == 0:
                # Apply sigmoid to get probabilities and binarize for the mask
                pred_probs = torch.sigmoid(output_logits)
                pred_mask = (pred_probs > 0.5).float() # Threshold at 0.5

                # Convert tensors to numpy arrays for visualization
                # Use current_img_batch and current_mask_batch directly
                img_np = current_img_batch.squeeze().cpu().numpy()
                mask_np = current_mask_batch.squeeze().cpu().numpy()
                pred_np = pred_mask.squeeze().cpu().numpy()

                # Ensure img_np, mask_np, pred_np are 3D (D, H, W) for slicing.
                while img_np.ndim > 3:
                    img_np = np.squeeze(img_np, axis=0)
                while mask_np.ndim > 3:
                    mask_np = np.squeeze(mask_np, axis=0)
                while pred_np.ndim > 3:
                    pred_np = np.squeeze(pred_np, axis=0)

                # Select a middle slice for visualization
                if img_np.ndim == 3:
                    slice_idx = img_np.shape[0] // 2
                elif img_np.ndim == 2:
                    slice_idx = None
                    print("Warning: Image array is 2D. Displaying the whole image without slicing.")
                else:
                    print(f"Error: Unexpected image dimensions ({img_np.shape}) for visualization. Expected 2D or 3D.")
                    continue # Skip visualization if dimensions are wrong

                # Plotting
                fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='black')
                fig.suptitle('Segmentation Result Comparison', fontsize=16, color='white')

                # Original Image
                if slice_idx is not None:
                    axes[0].imshow(img_np[slice_idx], cmap='gray')
                    axes[0].set_title(f'Original Image (Slice {slice_idx})', color='white')
                else:
                    axes[0].imshow(img_np, cmap='gray')
                    axes[0].set_title('Original Image', color='white')
                axes[0].axis('off')

                # Ground Truth Mask (overlayed)
                if slice_idx is not None:
                    axes[1].imshow(img_np[slice_idx], cmap='gray')
                    axes[1].imshow(mask_np[slice_idx], cmap='Greens', alpha=0.5)
                else:
                    axes[1].imshow(img_np, cmap='gray')
                    axes[1].imshow(mask_np, cmap='Greens', alpha=0.5)
                axes[1].set_title('Ground Truth Mask', color='white')
                axes[1].axis('off')

                # Predicted Mask (overlayed)
                if slice_idx is not None:
                    axes[2].imshow(img_np[slice_idx], cmap='gray')
                    axes[2].imshow(pred_np[slice_idx], cmap='Reds', alpha=0.5)
                else:
                    axes[2].imshow(img_np, cmap='gray')
                    axes[2].imshow(pred_np, cmap='Reds', alpha=0.5)
                axes[2].set_title('Predicted Mask', color='white')
                axes[2].axis('off')

                plt.tight_layout()
                save_path = os.path.join(output_dir, "prediction_visualization.png")
                plt.savefig(save_path, facecolor='black')
                print(f"✅ Visualization saved to: {save_path}")
                plt.close()

        if num_samples_tested > 0:
            avg_test_dice = total_test_dice / num_samples_tested
            avg_test_iou = total_test_iou / num_samples_tested
            print(f"\n--- Test Set Metrics ---")
            print(f"Average Test Dice Score: {avg_test_dice:.4f}")
            print(f"Average Test IoU Score: {avg_test_iou:.4f}")
        else:
            print("No samples processed in the test set.")


# ==============================================================================
# main.py content (now the main execution block)
# ==============================================================================
def main():
    """Main function to run the entire pipeline."""
    # --- Setup ---
    print(f"Using device: {DEVICE}")

    # Create output directory in Google Drive
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory ensured: {OUTPUT_DIR}")

    # --- Step 1: Get data file paths ---
    train_files, val_files, test_files = get_data_splits(DATA_DIR)

    # --- Step 2: Create Datasets and DataLoaders ---
    train_dataset = SimpleNpyDataset(train_files)
    val_dataset = SimpleNpyDataset(val_files)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- Step 3: Initialize Model, Loss, and Optimizer ---
    print("\n--- Initializing Model ---")
    model = LightUNet3D(in_channels=1, out_channels=NUM_CLASSES).to(DEVICE)
    print(f"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # --- Step 4: Train the model ---
    model_save_path = os.path.join(OUTPUT_DIR, MODEL_SAVE_NAME)
    train_model(
        model, train_loader, val_loader, optimizer, criterion,
        EPOCHS, DEVICE, model_save_path
    )

    # --- Step 5: Run prediction and visualize results on the test set ---
    if test_files:
        test_dataset = SimpleNpyDataset(test_files)
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        predict_and_visualize(model, test_loader, DEVICE, model_save_path, OUTPUT_DIR)
    else:
        print("\n--- Skipping visualization as no files were allocated for the test set. ---")

    print("\n[COMPLETE] Script finished.")

if __name__ == "__main__":
    main()

import os
import glob
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import time

# ==============================================================================
# config.py content
# ==============================================================================
DATA_DIR = "/content/drive/MyDrive/Colab.data/tubitk folder"
OUTPUT_FOLDER_NAME = "SimpleOutput_SegmentationVersionTwo" # New: Folder name for output in Google Drive
OUTPUT_DIR = os.path.join("/content/drive/MyDrive", OUTPUT_FOLDER_NAME) # Dynamically create Google Drive output path

# --- 2. MODEL & TRAINING PARAMETERS ---
# The shape of your .npy arrays (Height, Width, Depth)
INPUT_SHAPE = (128, 128, 64)
BATCH_SIZE = 1
EPOCHS = 50
LEARNING_RATE = 1e-4
NUM_CLASSES = 1 # Set to 1 for binary segmentation (background vs. foreground).
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_SAVE_NAME = "light_unet_best.pth"

# ==============================================================================
# metrics.py content (New Section)
# ==============================================================================

def dice_score(preds, targets, smooth=1e-6):
    """
    Calculates the Dice Similarity Coefficient (DSC).
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: Dice score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum()

    dice = (2. * intersection + smooth) / (union + smooth)
    return dice

def iou_score(preds, targets, smooth=1e-6):
    """
    Calculates the Intersection Over Union (IoU) or Jaccard Index.
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: IoU score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum() - intersection # Union = A + B - Intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou

# You can add more metrics here if needed, e.g., accuracy, sensitivity, specificity
def calculate_metrics(outputs, masks):
    """
    Calculates Dice and IoU scores for a batch of predictions and ground truths.
    Args:
        outputs (torch.Tensor): Raw model outputs (logits).
        masks (torch.Tensor): Ground truth masks.
    Returns:
        tuple: (average_dice, average_iou)
    """
    # Binarize predictions
    preds = (torch.sigmoid(outputs) > 0.5).float()

    batch_dice = []
    batch_iou = []

    for i in range(preds.shape[0]): # Iterate over batch dimension
        d = dice_score(preds[i], masks[i])
        j = iou_score(preds[i], masks[i])
        batch_dice.append(d.item())
        batch_iou.append(j.item())

    return np.mean(batch_dice), np.mean(batch_iou)


# ==============================================================================
# dataset.py content
# ==============================================================================
def get_data_splits(data_dir):
    """
    Finds image and mask .npy files, pairs them, and splits them into
    training, validation, and test sets.

    Args:
        data_dir (str): The path to the directory containing the .npy files.

    Returns:
        tuple: A tuple containing three lists: train_files, val_files, test_files.
               Each list contains (image_path, mask_path) tuples.
    """
    print("\n--- Finding and splitting data files ---")
    print(f"DEBUG: 'data_dir' received: '{data_dir}'")

    # Step 1: Verify the data directory exists
    if not os.path.isdir(data_dir):
        print(f"ERROR: The directory '{data_dir}' does not exist.")
        raise FileNotFoundError(f"Directory '{data_dir}' not found. Please ensure the path is correct and exists on your system.")

    # Step 2: Find all image files using a glob pattern
    search_pattern = os.path.join(data_dir, "*_cropped_img.npy")
    print(f"DEBUG: Glob search pattern: '{search_pattern}'")

    all_image_files = sorted(glob.glob(search_pattern))
    print(f"Found {len(all_image_files)} image files.")
    print(f"DEBUG: All image files found by glob: {all_image_files[:5]}")

    if not all_image_files:
        raise FileNotFoundError(f"No *_cropped_img.npy files found in '{data_dir}' using pattern '{search_pattern}'. Check file names or directory.")

    # Step 3: Generate expected mask file paths from image file paths
    all_mask_files = [f.replace("_img.npy", "_mask.npy") for f in all_image_files]
    print(f"Generated {len(all_mask_files)} expected mask file paths.")

    print("🧾 Sample image files found:", all_image_files[:2])
    print("🧾 Sample mask files expected:", all_mask_files[:2])

    # Step 4: Pair image and mask files, checking for mask file existence
    paired_files = []
    for img, mask in zip(all_image_files, all_mask_files):
        if os.path.exists(mask):
            paired_files.append((img, mask))
        else:
            print(f" Missing mask file: {mask}")

    print(f"DEBUG: Number of valid image-mask pairs found: {len(paired_files)}")

    if not paired_files:
        raise FileNotFoundError(f"No valid image-mask pairs found in '{data_dir}'. Check file names.")

    # Step 5: Shuffle the paired files and split into train, validation, and test sets
    random.seed(42) # For reproducibility
    random.shuffle(paired_files)

    total_files = len(paired_files)
    train_split_idx = int(total_files * 0.8)
    val_split_idx = int(total_files * 0.9)

    train_files = paired_files[:train_split_idx]
    val_files = paired_files[train_split_idx:val_split_idx]
    test_files = paired_files[val_split_idx:]

    print(f"✅ Total pairs: {total_files} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}")
    return train_files, val_files, test_files


class SimpleNpyDataset(Dataset):
    """
    A PyTorch Dataset class to load pre-processed .npy image and mask files.
    It handles loading, squeezing (if necessary), casting to float32,
    and reshaping to PyTorch's (C, D, H, W) format for 3D data, or (C, H, W) for 2D.
    """
    def __init__(self, file_pairs):
        """
        Initializes the dataset with lists of image and mask file paths.

        Args:
            file_pairs (list): A list of (image_path, mask_path) tuples.
        """
        self.image_paths = [pair[0] for pair in file_pairs]
        self.mask_paths = [pair[1] for pair in file_pairs]
        print(f"🔍 Initialized dataset with {len(self.image_paths)} samples.")

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.image_paths)

    def __getitem__(self, idx):
        """
        Loads and processes an image-mask pair at a given index.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: A tuple containing the processed image tensor and mask tensor.
                   Expected output shape for 3D data is (C, D, H, W).
        """
        # Load numpy arrays
        img = np.load(self.image_paths[idx])
        mask = np.load(self.mask_paths[idx])

        # Squeeze out singleton dimensions, e.g., (1, D, H, W) -> (D, H, W)
        if img.ndim == 4 and img.shape[0] == 1:
            img = np.squeeze(img, axis=0)
        if mask.ndim == 4 and mask.shape[0] == 1:
            mask = np.squeeze(mask, axis=0)

        # Binarize mask and ensure float32 type
        mask = (mask > 0).astype(np.float32)

        # Convert numpy arrays to PyTorch tensors and ensure float type
        img_tensor = torch.from_numpy(img).float()
        mask_tensor = torch.from_numpy(mask).float()

        # Reshape for PyTorch: (D, H, W) -> (C, D, H, W) for 3D grayscale
        if img_tensor.ndim == 3: # Assuming it's (Depth, Height, Width) for 3D grayscale
            img_tensor = img_tensor.unsqueeze(0)
        elif img_tensor.ndim == 2: # Assuming (H, W) for 2D grayscale
            img_tensor = img_tensor.unsqueeze(0)

        if mask_tensor.ndim == 3:
            mask_tensor = mask_tensor.unsqueeze(0)
        elif mask_tensor.ndim == 2:
            mask_tensor = mask_tensor.unsqueeze(0)

        return img_tensor, mask_tensor

# ==============================================================================
# model.py content
# ==============================================================================
def double_conv_block(in_channels, out_channels):
    """A block of two 3D convolutions with instance norm and ReLU."""
    return nn.Sequential(
        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
    )

class LightUNet3D(nn.Module):
    """A lighter version of the 3D U-Net for faster training."""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # Encoder
        self.enc1 = double_conv_block(in_channels, 16)
        self.pool1 = nn.MaxPool3d(2)
        self.enc2 = double_conv_block(16, 32)
        self.pool2 = nn.MaxPool3d(2)
        # Bottleneck
        self.bottleneck = double_conv_block(32, 64)
        # Decoder
        self.up2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.dec2 = double_conv_block(64, 32)
        self.up1 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)
        self.dec1 = double_conv_block(32, 16)
        # Output
        self.out_conv = nn.Conv3d(16, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bottleneck(self.pool2(e2))

        d2 = self.up2(b)
        if d2.shape[-3:] != e2.shape[-3:]:
            diffD = e2.shape[2] - d2.shape[2]
            diffH = e2.shape[3] - d2.shape[3]
            diffW = e2.shape[4] - d2.shape[4]
            e2 = e2[:, :, diffD // 2 : e2.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e2.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e2.shape[4] - diffW + diffW // 2]
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        if d1.shape[-3:] != e1.shape[-3:]:
            diffD = e1.shape[2] - d1.shape[2]
            diffH = e1.shape[3] - d1.shape[3]
            diffW = e1.shape[4] - d1.shape[4]
            e1 = e1[:, :, diffD // 2 : e1.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e1.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e1.shape[4] - diffW + diffW // 2]
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        return self.out_conv(d1)

# ==============================================================================
# train.py content
# ==============================================================================
def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_save_path):
    """Main training and validation loop."""
    print("\n--- Starting Model Training ---")
    best_val_loss = float('inf')

    for epoch in range(epochs):
        start_time = time.time()

        # --- Training phase ---
        model.train()
        total_train_loss = 0

        for imgs, masks in train_loader:
            imgs, masks = imgs.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # --- Validation phase ---
        model.eval()
        total_val_loss = 0
        total_val_dice = 0
        total_val_iou = 0

        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs, masks = imgs.to(device), masks.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, masks)
                total_val_loss += loss.item()

                # Calculate metrics for validation
                dice, iou = calculate_metrics(outputs, masks)
                total_val_dice += dice
                total_val_iou += iou

        avg_val_loss = total_val_loss / len(val_loader)
        avg_val_dice = total_val_dice / len(val_loader)
        avg_val_iou = total_val_iou / len(val_loader)

        # --- Logging and Saving ---
        elapsed_time = time.time() - start_time
        print(f"Epoch {epoch+1:02}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Val Dice: {avg_val_dice:.4f} | "
              f"Val IoU: {avg_val_iou:.4f} | "
              f"Time: {elapsed_time:.2f}s")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f"  🎉 New best model saved with Val Loss: {best_val_loss:.4f}")

    print("\n--- Model Training Finished ---")

# ==============================================================================
# predict.py content
# ==============================================================================
# ==============================================================================
# predict.py content
# ==============================================================================
def predict_and_visualize(model, loader, device, model_path, output_dir):
    """Runs prediction on a single test sample and saves a visualization."""
    print("\n--- Running Prediction and Visualization ---")

    # Load the best model weights
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Successfully loaded model from: {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}. Cannot perform prediction.")
        return
    except Exception as e:
        print(f"Error loading model state_dict: {e}")
        return

    model.eval() # Set model to evaluation mode

    total_test_dice = 0
    total_test_iou = 0
    num_samples_tested = 0

    with torch.no_grad():
        # Iterate through the entire test loader for comprehensive metric calculation
        for i, (current_img_batch, current_mask_batch) in enumerate(loader): # Renamed for clarity
            # Move tensors to the specified device (CPU/GPU)
            current_img_batch, current_mask_batch = current_img_batch.to(device), current_mask_batch.to(device)

            output_logits = model(current_img_batch)

            # Calculate metrics
            dice, iou = calculate_metrics(output_logits, current_mask_batch)
            total_test_dice += dice
            total_test_iou += iou
            num_samples_tested += 1

            # Only visualize the first sample
            if i == 0:
                # Apply sigmoid to get probabilities and binarize for the mask
                pred_probs = torch.sigmoid(output_logits)
                pred_mask = (pred_probs > 0.5).float() # Threshold at 0.5

                # Convert tensors to numpy arrays for visualization
                # Use current_img_batch and current_mask_batch directly
                img_np = current_img_batch.squeeze().cpu().numpy()
                mask_np = current_mask_batch.squeeze().cpu().numpy()
                pred_np = pred_mask.squeeze().cpu().numpy()

                # Ensure img_np, mask_np, pred_np are 3D (D, H, W) for slicing.
                while img_np.ndim > 3:
                    img_np = np.squeeze(img_np, axis=0)
                while mask_np.ndim > 3:
                    mask_np = np.squeeze(mask_np, axis=0)
                while pred_np.ndim > 3:
                    pred_np = np.squeeze(pred_np, axis=0)

                # Select a middle slice for visualization
                if img_np.ndim == 3:
                    slice_idx = img_np.shape[0] // 2
                elif img_np.ndim == 2:
                    slice_idx = None
                    print("Warning: Image array is 2D. Displaying the whole image without slicing.")
                else:
                    print(f"Error: Unexpected image dimensions ({img_np.shape}) for visualization. Expected 2D or 3D.")
                    continue # Skip visualization if dimensions are wrong

                # Plotting
                fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='black')
                fig.suptitle('Segmentation Result Comparison', fontsize=16, color='white')

                # Original Image
                if slice_idx is not None:
                    axes[0].imshow(img_np[slice_idx], cmap='gray')
                    axes[0].set_title(f'Original Image (Slice {slice_idx})', color='white')
                else:
                    axes[0].imshow(img_np, cmap='gray')
                    axes[0].set_title('Original Image', color='white')
                axes[0].axis('off')

                # Ground Truth Mask (overlayed)
                if slice_idx is not None:
                    axes[1].imshow(img_np[slice_idx], cmap='gray')
                    axes[1].imshow(mask_np[slice_idx], cmap='Greens', alpha=0.5)
                else:
                    axes[1].imshow(img_np, cmap='gray')
                    axes[1].imshow(mask_np, cmap='Greens', alpha=0.5)
                axes[1].set_title('Ground Truth Mask', color='white')
                axes[1].axis('off')

                # Predicted Mask (overlayed)
                if slice_idx is not None:
                    axes[2].imshow(img_np[slice_idx], cmap='gray')
                    axes[2].imshow(pred_np[slice_idx], cmap='Reds', alpha=0.5)
                else:
                    axes[2].imshow(img_np, cmap='gray')
                    axes[2].imshow(pred_np, cmap='Reds', alpha=0.5)
                axes[2].set_title('Predicted Mask', color='white')
                axes[2].axis('off')

                plt.tight_layout()
                save_path = os.path.join(output_dir, "prediction_visualization.png")
                plt.savefig(save_path, facecolor='black')
                print(f"✅ Visualization saved to: {save_path}")
                plt.close()

        if num_samples_tested > 0:
            avg_test_dice = total_test_dice / num_samples_tested
            avg_test_iou = total_test_iou / num_samples_tested
            print(f"\n--- Test Set Metrics ---")
            print(f"Average Test Dice Score: {avg_test_dice:.4f}")
            print(f"Average Test IoU Score: {avg_test_iou:.4f}")
        else:
            print("No samples processed in the test set.")


# ==============================================================================
# main.py content (now the main execution block)
# ==============================================================================
def main():
    """Main function to run the entire pipeline."""
    # --- Setup ---
    print(f"Using device: {DEVICE}")

    # Create output directory in Google Drive
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory ensured: {OUTPUT_DIR}")

    # --- Step 1: Get data file paths ---
    train_files, val_files, test_files = get_data_splits(DATA_DIR)

    # --- Step 2: Create Datasets and DataLoaders ---
    train_dataset = SimpleNpyDataset(train_files)
    val_dataset = SimpleNpyDataset(val_files)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- Step 3: Initialize Model, Loss, and Optimizer ---
    print("\n--- Initializing Model ---")
    model = LightUNet3D(in_channels=1, out_channels=NUM_CLASSES).to(DEVICE)
    print(f"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # --- Step 4: Train the model ---
    model_save_path = os.path.join(OUTPUT_DIR, MODEL_SAVE_NAME)
    train_model(
        model, train_loader, val_loader, optimizer, criterion,
        EPOCHS, DEVICE, model_save_path
    )

    # --- Step 5: Run prediction and visualize results on the test set ---
    if test_files:
        test_dataset = SimpleNpyDataset(test_files)
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        predict_and_visualize(model, test_loader, DEVICE, model_save_path, OUTPUT_DIR)
    else:
        print("\n--- Skipping visualization as no files were allocated for the test set. ---")

    print("\n[COMPLETE] Script finished.")

if __name__ == "__main__":
    main()

# Mount Google Drive and list all folder paths
from google.colab import drive
import os

drive.mount('/content/drive')
root_dir = '/content/drive/MyDrive'

for dirpath, dirnames, filenames in os.walk(root_dir):
    print(dirpath)

import os
import glob
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import time

# ==============================================================================
# config.py content
# ==============================================================================
DATA_DIR = "/content/drive/MyDrive/Colab.data/tubitk folder"
OUTPUT_FOLDER_NAME = "SimpleOutput_SegmentationVersionThree" # New: Folder name for output in Google Drive
OUTPUT_DIR = os.path.join("/content/drive/MyDrive", OUTPUT_FOLDER_NAME) # Dynamically create Google Drive output path

# --- 2. MODEL & TRAINING PARAMETERS ---
# The shape of your .npy arrays (Height, Width, Depth)
INPUT_SHAPE = (128, 128, 64)
BATCH_SIZE = 1
EPOCHS = 100
LEARNING_RATE = 1e-4
NUM_CLASSES = 1 # Set to 1 for binary segmentation (background vs. foreground).
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_SAVE_NAME = "light_unet_best.pth"

# ==============================================================================
# metrics.py content (New Section)
# ==============================================================================

def dice_score(preds, targets, smooth=1e-6):
    """
    Calculates the Dice Similarity Coefficient (DSC).
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: Dice score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum()

    dice = (2. * intersection + smooth) / (union + smooth)
    return dice

def iou_score(preds, targets, smooth=1e-6):
    """
    Calculates the Intersection Over Union (IoU) or Jaccard Index.
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: IoU score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum() - intersection # Union = A + B - Intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou

# You can add more metrics here if needed, e.g., accuracy, sensitivity, specificity
def calculate_metrics(outputs, masks):
    """
    Calculates Dice and IoU scores for a batch of predictions and ground truths.
    Args:
        outputs (torch.Tensor): Raw model outputs (logits).
        masks (torch.Tensor): Ground truth masks.
    Returns:
        tuple: (average_dice, average_iou)
    """
    # Binarize predictions
    preds = (torch.sigmoid(outputs) > 0.5).float()

    batch_dice = []
    batch_iou = []

    for i in range(preds.shape[0]): # Iterate over batch dimension
        d = dice_score(preds[i], masks[i])
        j = iou_score(preds[i], masks[i])
        batch_dice.append(d.item())
        batch_iou.append(j.item())

    return np.mean(batch_dice), np.mean(batch_iou)


# ==============================================================================
# dataset.py content
# ==============================================================================
def get_data_splits(data_dir):
    """
    Finds image and mask .npy files, pairs them, and splits them into
    training, validation, and test sets.

    Args:
        data_dir (str): The path to the directory containing the .npy files.

    Returns:
        tuple: A tuple containing three lists: train_files, val_files, test_files.
               Each list contains (image_path, mask_path) tuples.
    """
    print("\n--- Finding and splitting data files ---")
    print(f"DEBUG: 'data_dir' received: '{data_dir}'")

    # Step 1: Verify the data directory exists
    if not os.path.isdir(data_dir):
        print(f"ERROR: The directory '{data_dir}' does not exist.")
        raise FileNotFoundError(f"Directory '{data_dir}' not found. Please ensure the path is correct and exists on your system.")

    # Step 2: Find all image files using a glob pattern
    search_pattern = os.path.join(data_dir, "*_cropped_img.npy")
    print(f"DEBUG: Glob search pattern: '{search_pattern}'")

    all_image_files = sorted(glob.glob(search_pattern))
    print(f"Found {len(all_image_files)} image files.")
    print(f"DEBUG: All image files found by glob: {all_image_files[:5]}")

    if not all_image_files:
        raise FileNotFoundError(f"No *_cropped_img.npy files found in '{data_dir}' using pattern '{search_pattern}'. Check file names or directory.")

    # Step 3: Generate expected mask file paths from image file paths
    all_mask_files = [f.replace("_img.npy", "_mask.npy") for f in all_image_files]
    print(f"Generated {len(all_mask_files)} expected mask file paths.")

    print("🧾 Sample image files found:", all_image_files[:2])
    print("🧾 Sample mask files expected:", all_mask_files[:2])

    # Step 4: Pair image and mask files, checking for mask file existence
    paired_files = []
    for img, mask in zip(all_image_files, all_mask_files):
        if os.path.exists(mask):
            paired_files.append((img, mask))
        else:
            print(f" Missing mask file: {mask}")

    print(f"DEBUG: Number of valid image-mask pairs found: {len(paired_files)}")

    if not paired_files:
        raise FileNotFoundError(f"No valid image-mask pairs found in '{data_dir}'. Check file names.")

    # Step 5: Shuffle the paired files and split into train, validation, and test sets
    random.seed(42) # For reproducibility
    random.shuffle(paired_files)

    total_files = len(paired_files)
    train_split_idx = int(total_files * 0.8)
    val_split_idx = int(total_files * 0.9)

    train_files = paired_files[:train_split_idx]
    val_files = paired_files[train_split_idx:val_split_idx]
    test_files = paired_files[val_split_idx:]

    print(f"✅ Total pairs: {total_files} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}")
    return train_files, val_files, test_files


class SimpleNpyDataset(Dataset):
    """
    A PyTorch Dataset class to load pre-processed .npy image and mask files.
    It handles loading, squeezing (if necessary), casting to float32,
    and reshaping to PyTorch's (C, D, H, W) format for 3D data, or (C, H, W) for 2D.
    """
    def __init__(self, file_pairs):
        """
        Initializes the dataset with lists of image and mask file paths.

        Args:
            file_pairs (list): A list of (image_path, mask_path) tuples.
        """
        self.image_paths = [pair[0] for pair in file_pairs]
        self.mask_paths = [pair[1] for pair in file_pairs]
        print(f"🔍 Initialized dataset with {len(self.image_paths)} samples.")

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.image_paths)

    def __getitem__(self, idx):
        """
        Loads and processes an image-mask pair at a given index.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: A tuple containing the processed image tensor and mask tensor.
                   Expected output shape for 3D data is (C, D, H, W).
        """
        # Load numpy arrays
        img = np.load(self.image_paths[idx])
        mask = np.load(self.mask_paths[idx])

        # Squeeze out singleton dimensions, e.g., (1, D, H, W) -> (D, H, W)
        if img.ndim == 4 and img.shape[0] == 1:
            img = np.squeeze(img, axis=0)
        if mask.ndim == 4 and mask.shape[0] == 1:
            mask = np.squeeze(mask, axis=0)

        # Binarize mask and ensure float32 type
        mask = (mask > 0).astype(np.float32)

        # Convert numpy arrays to PyTorch tensors and ensure float type
        img_tensor = torch.from_numpy(img).float()
        mask_tensor = torch.from_numpy(mask).float()

        # Reshape for PyTorch: (D, H, W) -> (C, D, H, W) for 3D grayscale
        if img_tensor.ndim == 3: # Assuming it's (Depth, Height, Width) for 3D grayscale
            img_tensor = img_tensor.unsqueeze(0)
        elif img_tensor.ndim == 2: # Assuming (H, W) for 2D grayscale
            img_tensor = img_tensor.unsqueeze(0)

        if mask_tensor.ndim == 3:
            mask_tensor = mask_tensor.unsqueeze(0)
        elif mask_tensor.ndim == 2:
            mask_tensor = mask_tensor.unsqueeze(0)

        return img_tensor, mask_tensor

# ==============================================================================
# model.py content
# ==============================================================================
def double_conv_block(in_channels, out_channels):
    """A block of two 3D convolutions with instance norm and ReLU."""
    return nn.Sequential(
        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
    )

class LightUNet3D(nn.Module):
    """A lighter version of the 3D U-Net for faster training."""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # Encoder
        self.enc1 = double_conv_block(in_channels, 16)
        self.pool1 = nn.MaxPool3d(2)
        self.enc2 = double_conv_block(16, 32)
        self.pool2 = nn.MaxPool3d(2)
        # Bottleneck
        self.bottleneck = double_conv_block(32, 64)
        # Decoder
        self.up2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.dec2 = double_conv_block(64, 32)
        self.up1 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)
        self.dec1 = double_conv_block(32, 16)
        # Output
        self.out_conv = nn.Conv3d(16, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bottleneck(self.pool2(e2))

        d2 = self.up2(b)
        if d2.shape[-3:] != e2.shape[-3:]:
            diffD = e2.shape[2] - d2.shape[2]
            diffH = e2.shape[3] - d2.shape[3]
            diffW = e2.shape[4] - d2.shape[4]
            e2 = e2[:, :, diffD // 2 : e2.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e2.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e2.shape[4] - diffW + diffW // 2]
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        if d1.shape[-3:] != e1.shape[-3:]:
            diffD = e1.shape[2] - d1.shape[2]
            diffH = e1.shape[3] - d1.shape[3]
            diffW = e1.shape[4] - d1.shape[4]
            e1 = e1[:, :, diffD // 2 : e1.shape[2] - diffD + diffD // 2,
                       diffH // 2 : e1.shape[3] - diffH + diffH // 2,
                       diffW // 2 : e1.shape[4] - diffW + diffW // 2]
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        return self.out_conv(d1)

# ==============================================================================
# train.py content
# ==============================================================================
def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_save_path):
    """Main training and validation loop."""
    print("\n--- Starting Model Training ---")
    best_val_loss = float('inf')

    for epoch in range(epochs):
        start_time = time.time()

        # --- Training phase ---
        model.train()
        total_train_loss = 0

        for imgs, masks in train_loader:
            imgs, masks = imgs.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # --- Validation phase ---
        model.eval()
        total_val_loss = 0
        total_val_dice = 0
        total_val_iou = 0

        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs, masks = imgs.to(device), masks.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, masks)
                total_val_loss += loss.item()

                # Calculate metrics for validation
                dice, iou = calculate_metrics(outputs, masks)
                total_val_dice += dice
                total_val_iou += iou

        avg_val_loss = total_val_loss / len(val_loader)
        avg_val_dice = total_val_dice / len(val_loader)
        avg_val_iou = total_val_iou / len(val_loader)

        # --- Logging and Saving ---
        elapsed_time = time.time() - start_time
        print(f"Epoch {epoch+1:02}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Val Dice: {avg_val_dice:.4f} | "
              f"Val IoU: {avg_val_iou:.4f} | "
              f"Time: {elapsed_time:.2f}s")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f"  🎉 New best model saved with Val Loss: {best_val_loss:.4f}")

    print("\n--- Model Training Finished ---")

# ==============================================================================
# predict.py content
# ==============================================================================
# ==============================================================================
# predict.py content
# ==============================================================================
def predict_and_visualize(model, loader, device, model_path, output_dir):
    """Runs prediction on a single test sample and saves a visualization."""
    print("\n--- Running Prediction and Visualization ---")

    # Load the best model weights
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Successfully loaded model from: {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}. Cannot perform prediction.")
        return
    except Exception as e:
        print(f"Error loading model state_dict: {e}")
        return

    model.eval() # Set model to evaluation mode

    total_test_dice = 0
    total_test_iou = 0
    num_samples_tested = 0

    with torch.no_grad():
        # Iterate through the entire test loader for comprehensive metric calculation
        for i, (current_img_batch, current_mask_batch) in enumerate(loader): # Renamed for clarity
            # Move tensors to the specified device (CPU/GPU)
            current_img_batch, current_mask_batch = current_img_batch.to(device), current_mask_batch.to(device)

            output_logits = model(current_img_batch)

            # Calculate metrics
            dice, iou = calculate_metrics(output_logits, current_mask_batch)
            total_test_dice += dice
            total_test_iou += iou
            num_samples_tested += 1

            # Only visualize the first sample
            if i == 0:
                # Apply sigmoid to get probabilities and binarize for the mask
                pred_probs = torch.sigmoid(output_logits)
                pred_mask = (pred_probs > 0.5).float() # Threshold at 0.5

                # Convert tensors to numpy arrays for visualization
                # Use current_img_batch and current_mask_batch directly
                img_np = current_img_batch.squeeze().cpu().numpy()
                mask_np = current_mask_batch.squeeze().cpu().numpy()
                pred_np = pred_mask.squeeze().cpu().numpy()

                # Ensure img_np, mask_np, pred_np are 3D (D, H, W) for slicing.
                while img_np.ndim > 3:
                    img_np = np.squeeze(img_np, axis=0)
                while mask_np.ndim > 3:
                    mask_np = np.squeeze(mask_np, axis=0)
                while pred_np.ndim > 3:
                    pred_np = np.squeeze(pred_np, axis=0)

                # Select a middle slice for visualization
                if img_np.ndim == 3:
                    slice_idx = img_np.shape[0] // 2
                elif img_np.ndim == 2:
                    slice_idx = None
                    print("Warning: Image array is 2D. Displaying the whole image without slicing.")
                else:
                    print(f"Error: Unexpected image dimensions ({img_np.shape}) for visualization. Expected 2D or 3D.")
                    continue # Skip visualization if dimensions are wrong

                # Plotting
                fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='black')
                fig.suptitle('Segmentation Result Comparison', fontsize=16, color='white')

                # Original Image
                if slice_idx is not None:
                    axes[0].imshow(img_np[slice_idx], cmap='gray')
                    axes[0].set_title(f'Original Image (Slice {slice_idx})', color='white')
                else:
                    axes[0].imshow(img_np, cmap='gray')
                    axes[0].set_title('Original Image', color='white')
                axes[0].axis('off')

                # Ground Truth Mask (overlayed)
                if slice_idx is not None:
                    axes[1].imshow(img_np[slice_idx], cmap='gray')
                    axes[1].imshow(mask_np[slice_idx], cmap='Greens', alpha=0.5)
                else:
                    axes[1].imshow(img_np, cmap='gray')
                    axes[1].imshow(mask_np, cmap='Greens', alpha=0.5)
                axes[1].set_title('Ground Truth Mask', color='white')
                axes[1].axis('off')

                # Predicted Mask (overlayed)
                if slice_idx is not None:
                    axes[2].imshow(img_np[slice_idx], cmap='gray')
                    axes[2].imshow(pred_np[slice_idx], cmap='Reds', alpha=0.5)
                else:
                    axes[2].imshow(img_np, cmap='gray')
                    axes[2].imshow(pred_np, cmap='Reds', alpha=0.5)
                axes[2].set_title('Predicted Mask', color='white')
                axes[2].axis('off')

                plt.tight_layout()
                save_path = os.path.join(output_dir, "prediction_visualization.png")
                plt.savefig(save_path, facecolor='black')
                print(f"✅ Visualization saved to: {save_path}")
                plt.close()

        if num_samples_tested > 0:
            avg_test_dice = total_test_dice / num_samples_tested
            avg_test_iou = total_test_iou / num_samples_tested
            print(f"\n--- Test Set Metrics ---")
            print(f"Average Test Dice Score: {avg_test_dice:.4f}")
            print(f"Average Test IoU Score: {avg_test_iou:.4f}")
        else:
            print("No samples processed in the test set.")


# ==============================================================================
# main.py content (now the main execution block)
# ==============================================================================
def main():
    """Main function to run the entire pipeline."""
    # --- Setup ---
    print(f"Using device: {DEVICE}")

    # Create output directory in Google Drive
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory ensured: {OUTPUT_DIR}")

    # --- Step 1: Get data file paths ---
    train_files, val_files, test_files = get_data_splits(DATA_DIR)

    # --- Step 2: Create Datasets and DataLoaders ---
    train_dataset = SimpleNpyDataset(train_files)
    val_dataset = SimpleNpyDataset(val_files)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --- Step 3: Initialize Model, Loss, and Optimizer ---
    print("\n--- Initializing Model ---")
    model = LightUNet3D(in_channels=1, out_channels=NUM_CLASSES).to(DEVICE)
    print(f"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # --- Step 4: Train the model ---
    model_save_path = os.path.join(OUTPUT_DIR, MODEL_SAVE_NAME)
    train_model(
        model, train_loader, val_loader, optimizer, criterion,
        EPOCHS, DEVICE, model_save_path
    )

    # --- Step 5: Run prediction and visualize results on the test set ---
    if test_files:
        test_dataset = SimpleNpyDataset(test_files)
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        predict_and_visualize(model, test_loader, DEVICE, model_save_path, OUTPUT_DIR)
    else:
        print("\n--- Skipping visualization as no files were allocated for the test set. ---")

    print("\n[COMPLETE] Script finished.")

if __name__ == "__main__":
    main()

import os
import glob
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import time
from scipy.ndimage import rotate, zoom, shift # For 3D transformations

# ==============================================================================
# Step 0: Mount Google Drive (REQUIRED FIRST STEP IN COLAB)
# This addresses the "Mountpoint must not already contain files" error
# Make sure to run "Runtime -> Restart runtime" in Colab before executing this cell
# ==============================================================================
from google.colab import drive

# ==============================================================================
# config.py content
# ==============================================================================
# !!! IMPORTANT: VERIFY THIS PATH !!!
# Navigate to /content/drive/MyDrive/ in Colab's file browser and
# ensure '/Colab.data/tubitk folder' is the *exact* correct sub-path and spelling.
data_dir = "/Users/nedimyildirim/staj_ISU/tubitk folder folder" # <-- *** ADJUST THIS PATH IF NECESSARY ***

OUTPUT_FOLDER_NAME = "SimpleOutput_Segmentation"
OUTPUT_DIR = os.path.join("/Users/nedimyildirim/staj_ISU/tubitk folder", OUTPUT_FOLDER_NAME)

# --- 2. MODEL & TRAINING PARAMETERS ---
INPUT_SHAPE = (128, 128, 64) # This is used conceptually, actual input comes from dataset
BATCH_SIZE = 1
EPOCHS = 30
LEARNING_RATE = 1e-4
NUM_CLASSES = 1 # Set to 1 for binary segmentation (background vs. foreground).
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_SAVE_NAME = "light_unet_best.pth"

# ==============================================================================
# metrics.py content
# ==============================================================================
def dice_score(preds, targets, smooth=1e-6):
    """
    Calculates the Dice Similarity Coefficient (DSC).
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: Dice score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum()

    dice = (2. * intersection + smooth) / (union + smooth)
    return dice

def iou_score(preds, targets, smooth=1e-6):
    """
    Calculates the Intersection Over Union (IoU) or Jaccard Index.
    Args:
        preds (torch.Tensor): Predicted masks (binarized: 0 or 1).
        targets (torch.Tensor): Ground truth masks (binarized: 0 or 1).
        smooth (float): A small constant to avoid division by zero.
    Returns:
        float: IoU score.
    """
    preds = preds.float().flatten()
    targets = targets.float().flatten()

    intersection = (preds * targets).sum()
    union = preds.sum() + targets.sum() - intersection # Union = A + B - Intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou

def calculate_metrics(outputs, masks):
    """
    Calculates Dice and IoU scores for a batch of predictions and ground truths.
    Args:
        outputs (torch.Tensor): Raw model outputs (logits).
        masks (torch.Tensor): Ground truth masks.
    Returns:
        tuple: (average_dice, average_iou)
    """
    # Binarize predictions
    preds = (torch.sigmoid(outputs) > 0.5).float()

    batch_dice = []
    batch_iou = []

    for i in range(preds.shape[0]): # Iterate over batch dimension
        d = dice_score(preds[i], masks[i])
        j = iou_score(preds[i], masks[i])
        batch_dice.append(d.item())
        batch_iou.append(j.item())

    return np.mean(batch_dice), np.mean(batch_iou)


# ==============================================================================
# dataset.py content (with augmentation)
# ==============================================================================
def get_data_splits(data_dir):
    """
    Finds image and mask .npy files, pairs them, and splits them into
    training, validation, and test sets.

    Args:
        data_dir (str): The path to the directory containing the .npy files.

    Returns:
        tuple: A tuple containing three lists: train_files, val_files, test_files.
               Each list contains (image_path, mask_path) tuples.
    """
    print("\n--- Finding and splitting data files ---")
    print(f"DEBUG: 'data_dir' received: '{data_dir}'")

    # Step 1: Verify the data directory exists
    if not os.path.isdir(data_dir):
        print(f"ERROR: The directory '{data_dir}' does not exist.")
        raise FileNotFoundError(f"Directory '{data_dir}' not found. Please ensure the path is correct and exists on your system.")

    # Step 2: Find all image files using a glob pattern
    search_pattern = os.path.join(data_dir, "*_cropped_img.npy")
    print(f"DEBUG: Glob search pattern: '{search_pattern}'")

    all_image_files = sorted(glob.glob(search_pattern))
    print(f"Found {len(all_image_files)} image files.")
    print(f"DEBUG: All image files found by glob: {all_image_files[:5]}")

    if not all_image_files:
        raise FileNotFoundError(f"No *_cropped_img.npy files found in '{data_dir}' using pattern '{search_pattern}'. Check file names or directory.")

    # Step 3: Generate expected mask file paths from image file paths
    all_mask_files = [f.replace("_img.npy", "_mask.npy") for f in all_image_files]
    print(f"Generated {len(all_mask_files)} expected mask file paths.")

    print(" Sample image files found:", all_image_files[:2])
    print(" Sample mask files expected:", all_mask_files[:2])

    # Step 4: Pair image and mask files, checking for mask file existence
    paired_files = []
    for img, mask in zip(all_image_files, all_mask_files):
        if os.path.exists(mask):
            paired_files.append((img, mask))
        else:
            print(f" Missing mask file: {mask}")

    print(f"DEBUG: Number of valid image-mask pairs found: {len(paired_files)}")

    if not paired_files:
        raise FileNotFoundError(f"No valid image-mask pairs found in '{data_dir}'. Check file names.")

    # Step 5: Shuffle the paired files and split into train, validation, and test sets
    random.seed(42) # For reproducibility
    random.shuffle(paired_files)

    total_files = len(paired_files)
    train_split_idx = int(total_files * 0.8)
    val_split_idx = int(total_files * 0.9)

    train_files = paired_files[:train_split_idx]
    val_files = paired_files[train_split_idx:val_split_idx]
    test_files = paired_files[val_split_idx:]

    print(f"Total pairs: {total_files} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}")
    return train_files, val_files, test_files


class SimpleNpyDataset(Dataset):
    """
    A PyTorch Dataset class to load pre-processed .npy image and mask files.
    It handles loading, squeezing (if necessary), casting to float32,
    and reshaping to PyTorch's (C, D, H, W) format for 3D data, or (C, H, W) for 2D.
    Now includes an optional 'transform' argument for data augmentation.
    """
    def __init__(self, file_pairs, transform=None): # Added transform argument
        """
        Initializes the dataset with lists of image and mask file paths.

        Args:
            file_pairs (list): A list of (image_path, mask_path) tuples.
            transform (callable, optional): A function/callable that takes (image, mask)
                                            and returns augmented (image, mask) tuples.
        """
        self.image_paths = [pair[0] for pair in file_pairs]
        self.mask_paths = [pair[1] for pair in file_pairs]
        self.transform = transform # Store the transform
        print(f"🔍 Initialized dataset with {len(self.image_paths)} samples. Transform: {self.transform is not None}")

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.image_paths)

    def __getitem__(self, idx):
        """
        Loads and processes an image-mask pair at a given index.
        Applies transformations if 'transform' is provided.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            tuple: A tuple containing the processed image tensor and mask tensor.
                   Expected output shape for 3D data is (C, D, H, W).
        """
        # Load numpy arrays
        img = np.load(self.image_paths[idx])
        mask = np.load(self.mask_paths[idx])

        # Squeeze out singleton dimensions, e.g., (1, D, H, W) -> (D, H, W)
        # Ensure img and mask are 3D (D, H, W) before augmentation if they are originally 4D
        if img.ndim == 4 and img.shape[0] == 1:
            img = np.squeeze(img, axis=0)
        if mask.ndim == 4 and mask.shape[0] == 1:
            mask = np.squeeze(mask, axis=0)

        # Binarize mask before augmentation to ensure labels are discrete
        mask = (mask > 0).astype(np.float32)

        # Apply transformations if provided
        if self.transform:
            img, mask = self.transform(img, mask)

        # Convert numpy arrays to PyTorch tensors and ensure float type
        img_tensor = torch.from_numpy(img).float()
        mask_tensor = torch.from_numpy(mask).float()

        # Reshape for PyTorch: (D, H, W) -> (C, D, H, W) for 3D grayscale
        # Or (H, W) -> (C, H, W) for 2D grayscale
        # This unsqueeze should happen AFTER augmentation to ensure consistent dimensionality
        if img_tensor.ndim == 3: # Assuming it's (Depth, Height, Width) for 3D grayscale
            img_tensor = img_tensor.unsqueeze(0)
        elif img_tensor.ndim == 2: # Assuming (H, W) for 2D grayscale
            img_tensor = img_tensor.unsqueeze(0)

        if mask_tensor.ndim == 3:
            mask_tensor = mask_tensor.unsqueeze(0)
        elif mask_tensor.ndim == 2:
            mask_tensor = mask_tensor.unsqueeze(0)

        return img_tensor, mask_tensor


class Custom3DAugmentations:
    """
    A callable class for applying 3D data augmentation to image-mask pairs.
    Assumes input images/masks are 3D (D, H, W) numpy arrays.
    """
    def __init__(self, rotation_range=(-5, 5), zoom_range=(0.9, 1.1), shift_range=5, flip_prob=0.5,
                 brightness_contrast_prob=0.3, noise_prob=0.2):
        self.rotation_range = rotation_range # in degrees
        self.zoom_range = zoom_range # scaling factor
        self.shift_range = shift_range # max pixels to shift
        self.flip_prob = flip_prob
        self.brightness_contrast_prob = brightness_contrast_prob
        self.noise_prob = noise_prob

    def __call__(self, img, mask):
        # Ensure img and mask are numpy arrays and of the correct type for scipy
        img = img.astype(np.float32)
        mask = mask.astype(np.float32) # Keep mask as float for transformations, then re-binarize

        original_shape = img.shape # Store original shape for zoom padding/cropping

        # 1. Random Flip (3D - depth, height, width axes)
        if random.random() < self.flip_prob:
            axis_to_flip = random.choice([0, 1, 2])
            img = np.flip(img, axis=axis_to_flip).copy()
            mask = np.flip(mask, axis=axis_to_flip).copy()

        # 2. Random Rotation (3D)
        if self.rotation_range[0] != 0 or self.rotation_range[1] != 0:
            angle = random.uniform(self.rotation_range[0], self.rotation_range[1])
            axes = random.choice([(0,1), (0,2), (1,2)]) # (depth, height), (depth, width), (height, width)

            # Order = 1 for linear interpolation for image, 0 for nearest neighbor for mask
            img = rotate(img, angle, axes=axes, reshape=False, order=1, mode='reflect')
            mask = rotate(mask, angle, axes=axes, reshape=False, order=0, mode='reflect') # Nearest neighbor for mask

        # 3. Random Zoom (3D)
        if self.zoom_range[0] != 1 or self.zoom_range[1] != 1:
            zoom_factor = random.uniform(self.zoom_range[0], self.zoom_range[1])
            img = zoom(img, zoom_factor, order=1, mode='reflect')
            mask = zoom(mask, zoom_factor, order=0, mode='reflect')

            # Pad/crop back to original shape after zoom
            new_shape = img.shape

            # Calculate padding/cropping amounts for each dimension
            pad_d_before = max(0, (original_shape[0] - new_shape[0]) // 2)
            pad_d_after = max(0, original_shape[0] - new_shape[0] - pad_d_before)

            pad_h_before = max(0, (original_shape[1] - new_shape[1]) // 2)
            pad_h_after = max(0, original_shape[1] - new_shape[1] - pad_h_before)

            pad_w_before = max(0, (original_shape[2] - new_shape[2]) // 2)
            pad_w_after = max(0, original_shape[2] - new_shape[2] - pad_w_before)

            img = np.pad(img, ((pad_d_before, pad_d_after),
                               (pad_h_before, pad_h_after),
                               (pad_w_before, pad_w_after)), mode='constant')
            mask = np.pad(mask, ((pad_d_before, pad_d_after),
                                 (pad_h_before, pad_h_after),
                                 (pad_w_before, pad_w_after)), mode='constant')

            # If image became larger, crop it to the original size from the center
            crop_d_before = max(0, (new_shape[0] - original_shape[0]) // 2)
            crop_h_before = max(0, (new_shape[1] - original_shape[1]) // 2)
            crop_w_before = max(0, (new_shape[2] - original_shape[2]) // 2)

            img = img[crop_d_before : crop_d_before + original_shape[0],
                      crop_h_before : crop_h_before + original_shape[1],
                      crop_w_before : crop_w_before + original_shape[2]]
            mask = mask[crop_d_before : crop_d_before + original_shape[0],
                        crop_h_before : crop_h_before + original_shape[1],
                        crop_w_before : crop_w_before + original_shape[2]]


        # 4. Random Shift (3D)
        if self.shift_range > 0:
            shift_d = random.randint(-self.shift_range, self.shift_range)
            shift_h = random.randint(-self.shift_range, self.shift_range)
            shift_w = random.randint(-self.shift_range, self.shift_range)

            img = shift(img, (shift_d, shift_h, shift_w), order=1, mode='reflect')
            mask = shift(mask, (shift_d, shift_h, shift_w), order=0, mode='reflect') # Nearest neighbor

        # Re-binarize the mask after geometric transformations
        mask = (mask > 0.5).astype(np.float32) # Use 0.5 threshold after interpolation

        # Add intensity augmentations (only on image)
        # 5. Random Brightness/Contrast
        if random.random() < self.brightness_contrast_prob:
            brightness_factor = random.uniform(0.8, 1.2)
            contrast_factor = random.uniform(0.8, 1.2)
            img = img * contrast_factor + (brightness_factor - 1) * 0.5 # Simple linear adjustment
            img = np.clip(img, 0, 1) # Assuming input image is normalized 0-1

        # 6. Gaussian Noise
        if random.random() < self.noise_prob:
            noise = np.random.normal(0, 0.05, img.shape).astype(np.float32) # Mean 0, Std Dev 0.05
            img = img + noise
            img = np.clip(img, 0, 1)

        return img, mask


# ==============================================================================
# model.py content
# ==============================================================================
def double_conv_block(in_channels, out_channels):
    """A block of two 3D convolutions with instance norm and ReLU."""
    return nn.Sequential(
        nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
        nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
        nn.InstanceNorm3d(out_channels),
        nn.ReLU(inplace=True),
    )

class LightUNet3D(nn.Module):
    """A lighter version of the 3D U-Net for faster training."""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # Encoder
        self.enc1 = double_conv_block(in_channels, 16)
        self.pool1 = nn.MaxPool3d(2)
        self.enc2 = double_conv_block(16, 32)
        self.pool2 = nn.MaxPool3d(2)
        # Bottleneck
        self.bottleneck = double_conv_block(32, 64)
        # Decoder
        self.up2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)
        self.dec2 = double_conv_block(64, 32)
        self.up1 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)
        self.dec1 = double_conv_block(32, 16)
        # Output
        self.out_conv = nn.Conv3d(16, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bottleneck(self.pool2(e2))

        d2 = self.up2(b)
        if d2.shape[-3:] != e2.shape[-3:]:
            diffD = e2.shape[2] - d2.shape[2]
            diffH = e2.shape[3] - d2.shape[3]
            diffW = e2.shape[4] - d2.shape[4]
            # Center crop e2 if it's larger
            e2 = e2[:, :, diffD // 2 : e2.shape[2] - (diffD - diffD // 2),
                       diffH // 2 : e2.shape[3] - (diffH - diffH // 2),
                       diffW // 2 : e2.shape[4] - (diffW - diffW // 2)]
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        if d1.shape[-3:] != e1.shape[-3:]:
            diffD = e1.shape[2] - d1.shape[2]
            diffH = e1.shape[3] - d1.shape[3]
            diffW = e1.shape[4] - d1.shape[4]
            # Center crop e1 if it's larger
            e1 = e1[:, :, diffD // 2 : e1.shape[2] - (diffD - diffD // 2),
                       diffH // 2 : e1.shape[3] - (diffH - diffH // 2),
                       diffW // 2 : e1.shape[4] - (diffW - diffW // 2)]
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        return self.out_conv(d1)

# ==============================================================================
# train.py content
# ==============================================================================
def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_save_path):
    """Main training and validation loop."""
    print("\n--- Starting Model Training ---")
    best_val_loss = float('inf')

    for epoch in range(epochs):
        start_time = time.time()

        # --- Training phase ---
        model.train()
        total_train_loss = 0

        for imgs, masks in train_loader:
            imgs, masks = imgs.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        # --- Validation phase ---
        model.eval()
        total_val_loss = 0
        total_val_dice = 0
        total_val_iou = 0

        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs, masks = imgs.to(device), masks.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, masks)
                total_val_loss += loss.item()

                # Calculate metrics for validation
                dice, iou = calculate_metrics(outputs, masks)
                total_val_dice += dice
                total_val_iou += iou

        avg_val_loss = total_val_loss / len(val_loader)
        avg_val_dice = total_val_dice / len(val_loader)
        avg_val_iou = total_val_iou / len(val_loader)

        # --- Logging and Saving ---
        elapsed_time = time.time() - start_time
        print(f"Epoch {epoch+1:02}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f} | "
              f"Val Loss: {avg_val_loss:.4f} | "
              f"Val Dice: {avg_val_dice:.4f} | "
              f"Val IoU: {avg_val_iou:.4f} | "
              f"Time: {elapsed_time:.2f}s")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), model_save_path)
            print(f"  🎉 New best model saved with Val Loss: {best_val_loss:.4f}")

    print("\n--- Model Training Finished ---")

# ==============================================================================
# predict.py content
# ==============================================================================
def predict_and_visualize(model, loader, device, model_path, output_dir):
    """Runs prediction on a single test sample and saves a visualization."""
    print("\n--- Running Prediction and Visualization ---")

    # Load the best model weights
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"Successfully loaded model from: {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}. Cannot perform prediction.")
        return
    except Exception as e:
        print(f"Error loading model state_dict: {e}")
        return

    model.eval() # Set model to evaluation mode

    total_test_dice = 0
    total_test_iou = 0
    num_samples_tested = 0

    with torch.no_grad():
        # Iterate through the entire test loader for comprehensive metric calculation
        for i, (current_img_batch, current_mask_batch) in enumerate(loader):
            # Move tensors to the specified device (CPU/GPU)
            current_img_batch, current_mask_batch = current_img_batch.to(device), current_mask_batch.to(device)

            output_logits = model(current_img_batch)

            # Calculate metrics
            dice, iou = calculate_metrics(output_logits, current_mask_batch)
            total_test_dice += dice
            total_test_iou += iou
            num_samples_tested += 1

            # Only visualize the first sample
            if i == 0:
                # Apply sigmoid to get probabilities and binarize for the mask
                pred_probs = torch.sigmoid(output_logits)
                pred_mask = (pred_probs > 0.5).float() # Threshold at 0.5

                # Convert tensors to numpy arrays for visualization
                img_np = current_img_batch.squeeze().cpu().numpy()
                mask_np = current_mask_batch.squeeze().cpu().numpy()
                pred_np = pred_mask.squeeze().cpu().numpy()

                # Ensure img_np, mask_np, pred_np are 3D (D, H, W) for slicing.
                while img_np.ndim > 3:
                    img_np = np.squeeze(img_np, axis=0)
                while mask_np.ndim > 3:
                    mask_np = np.squeeze(mask_np, axis=0)
                while pred_np.ndim > 3:
                    pred_np = np.squeeze(pred_np, axis=0)


                # Select a middle slice for visualization
                if img_np.ndim == 3:
                    slice_idx = img_np.shape[0] // 2
                elif img_np.ndim == 2:
                    slice_idx = None
                    print("Warning: Image array is 2D. Displaying the whole image without slicing.")
                else:
                    print(f"Error: Unexpected image dimensions ({img_np.shape}) for visualization. Expected 2D or 3D.")
                    continue # Skip visualization if dimensions are wrong

                # Plotting
                fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='black')
                fig.suptitle('Segmentation Result Comparison', fontsize=16, color='white')

                # Original Image
                if slice_idx is not None:
                    axes[0].imshow(img_np[slice_idx], cmap='gray')
                    axes[0].set_title(f'Original Image (Slice {slice_idx})', color='white')
                else:
                    axes[0].imshow(img_np, cmap='gray')
                    axes[0].set_title('Original Image', color='white')
                axes[0].axis('off')

                # Ground Truth Mask (overlayed)
                if slice_idx is not None:
                    axes[1].imshow(img_np[slice_idx], cmap='gray')
                    axes[1].imshow(mask_np[slice_idx], cmap='Greens', alpha=0.5)
                else:
                    axes[1].imshow(img_np, cmap='gray')
                    axes[1].imshow(mask_np, cmap='Greens', alpha=0.5)
                axes[1].set_title('Ground Truth Mask', color='white')
                axes[1].axis('off')

                # Predicted Mask (overlayed)
                if slice_idx is not None:
                    axes[2].imshow(img_np[slice_idx], cmap='gray')
                    axes[2].imshow(pred_np[slice_idx], cmap='Reds', alpha=0.5)
                else:
                    axes[2].imshow(img_np, cmap='gray')
                    axes[2].imshow(pred_np, cmap='Reds', alpha=0.5)
                axes[2].set_title('Predicted Mask', color='white')
                axes[2].axis('off')

                plt.tight_layout()
                save_path = os.path.join(output_dir, "prediction_visualization.png")
                plt.savefig(save_path, facecolor='black')
                print(f"✅ Visualization saved to: {save_path}")
                plt.close()

        if num_samples_tested > 0:
            avg_test_dice = total_test_dice / num_samples_tested
            avg_test_iou = total_test_iou / num_samples_tested
            print(f"\n--- Test Set Metrics ---")
            print(f"Average Test Dice Score: {avg_test_dice:.4f}")
            print(f"Average Test IoU Score: {avg_test_iou:.4f}")
        else:
            print("No samples processed in the test set.")


# ==============================================================================
# main.py content (now the main execution block)
# ==============================================================================
def main():
    """Main function to run the entire pipeline."""
    # --- Setup ---
    print(f"Using device: {DEVICE}")

    # Create output directory in Google Drive
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Output directory ensured: {OUTPUT_DIR}")

    # --- Step 1: Get data file paths ---
    train_files, val_files, test_files = get_data_splits(DATA_DIR)

    # --- Step 2: Create Datasets and DataLoaders ---
    print("\n--- Initializing Datasets with Augmentation ---")

    # Initialize augmentation for training data
    # Adjust parameters based on your data and desired augmentation strength
    train_transform = Custom3DAugmentations(
        rotation_range=(-10, 10),  # Rotate up to +/- 10 degrees
        zoom_range=(0.8, 1.2),     # Zoom between 80% and 120%
        shift_range=10,            # Shift up to +/- 10 pixels
        flip_prob=0.5,             # 50% chance of flipping
        brightness_contrast_prob=0.3, # 30% chance for brightness/contrast
        noise_prob=0.2             # 20% chance for gaussian noise
    )
    # No augmentation for validation and test sets (important for fair evaluation)
    val_transform = None
    test_transform = None

    train_dataset = SimpleNpyDataset(train_files, transform=train_transform)
    val_dataset = SimpleNpyDataset(val_files, transform=val_transform)

    # Added num_workers and pin_memory for efficiency in DataLoader
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

    # --- Step 3: Initialize Model, Loss, and Optimizer ---
    print("\n--- Initializing Model ---")
    model = LightUNet3D(in_channels=1, out_channels=NUM_CLASSES).to(DEVICE)
    print(f"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.")

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # --- Step 4: Train the model ---
    model_save_path = os.path.join(OUTPUT_DIR, MODEL_SAVE_NAME)
    train_model(
        model, train_loader, val_loader, optimizer, criterion,
        EPOCHS, DEVICE, model_save_path
    )

    # --- Step 5: Run prediction and visualize results on the test set ---
    if test_files:
        test_dataset = SimpleNpyDataset(test_files, transform=test_transform) # No augmentation for test
        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
        predict_and_visualize(model, test_loader, DEVICE, model_save_path, OUTPUT_DIR)
    else:
        print("\n--- Skipping visualization as no files were allocated for the test set. ---")

    print("\n[COMPLETE] Script finished.")

if __name__ == "__main__":
    main()

from google.colab import drive
drive.mount('/content/drive')